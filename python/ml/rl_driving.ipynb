{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CarRacing-v2\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "env = DummyVecEnv([lambda: env]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = PPO(\"CnnPolicy\", env, verbose=0, n_epochs=100, learning_rate=0.001, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ppo_model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:200\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m approx_kl_divs \u001b[39m=\u001b[39m []\n\u001b[0;32m    199\u001b[0m \u001b[39m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[39mfor\u001b[39;00m rollout_data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_buffer\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[0;32m    201\u001b[0m     actions \u001b[39m=\u001b[39m rollout_data\u001b[39m.\u001b[39mactions\n\u001b[0;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mDiscrete):\n\u001b[0;32m    203\u001b[0m         \u001b[39m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:477\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    475\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    476\u001b[0m \u001b[39mwhile\u001b[39;00m start_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs:\n\u001b[1;32m--> 477\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_samples(indices[start_idx : start_idx \u001b[39m+\u001b[39;49m batch_size])\n\u001b[0;32m    478\u001b[0m     start_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:488\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_samples\u001b[39m(\n\u001b[0;32m    481\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    482\u001b[0m     batch_inds: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m    483\u001b[0m     env: Optional[VecNormalize] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    484\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RolloutBufferSamples:  \u001b[39m# type: ignore[signature-mismatch] #FIXME\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     data \u001b[39m=\u001b[39m (\n\u001b[0;32m    486\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[batch_inds],\n\u001b[0;32m    487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[batch_inds],\n\u001b[1;32m--> 488\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_probs[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    490\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvantages[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    491\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturns[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutBufferSamples(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_torch, data)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo_model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'ppo_driving_model')\n",
    "ppo_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ppo_model\n",
    "ppo_model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(ppo_model, env, n_eval_episodes=5, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
